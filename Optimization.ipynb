{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "from plots import plot_function \n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a general convex cost function $f$:\n",
    "\n",
    "\\begin{equation}\n",
    "f(\\mathbf{w}) = \\frac{1}{2} \\mathbf{w}^{\\top} \\mathsf{A}\\, \\mathbf{w}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathsf{A} \\in \\mathbb{R}^{2 \\times 2}$ is a positive semi-definite matrix. We want to minimize this function iteratively by making steps *downhill* via **gradient descent**:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w}_{i+1} \\leftarrow \\mathbf{w}_i - \\eta \\nabla_w f(\\mathbf{w}_i).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def f(w, A):\n",
    "    return (1/2) * jnp.dot(w.T, jnp.dot(A, w))\n",
    "\n",
    "# Gradient of the function\n",
    "df_dw = grad(f, argnums=0)\n",
    "\n",
    "np.random.seed(10)\n",
    "# Create a random symmetric positive definite matrix A \n",
    "A = np.random.rand(2, 2)\n",
    "A = jnp.dot(A, A.T)  \n",
    "A = jnp.array(A)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, ax3d = plot_function(np.linspace(-5, 5, 50), lambda w: f(w, A))\n",
    "\n",
    "# hyperparameters\n",
    "η = 0.5 # learning rate\n",
    "num_iter = 10 # number of iterations\n",
    "w = np.array([4.0,3.5]) # initial guess\n",
    "\n",
    "ax.plot(*w, marker='.', color='k', ms=15)\n",
    "\n",
    "for i in range(num_iter):\n",
    "\n",
    "    # we keep a copy of the previous version for plotting\n",
    "    w_old = np.copy(w)\n",
    "\n",
    "    # perform the GD update\n",
    "    w += -η*df_dw(w, A)\n",
    "\n",
    "    # plot\n",
    "    ax.plot([w_old[0], w[0]], [w_old[1], w[1]], marker='.', linestyle='-', color='k',lw=1)\n",
    "    ax3d.plot([w_old[0], w[0]], [w_old[1], w[1]], [f(w_old,A),f(w,A)], marker='.', linestyle='-', color='k',lw=1, zorder=100)\n",
    "\n",
    "    ax.set_title(f'$i={i}, w=[{w[0]:.2f},{w[1]:.2f}]$' + '\\n' + f'$f(w) = {f(w,A):.6f}$', fontsize=14);\n",
    "\n",
    "    display.display(fig)\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Methods\n",
    "\n",
    "## Gradient Descent with Momentum\n",
    " We modify our above algorithm to include *momentum* (or memory):\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{v}_i &\\leftarrow \\beta \\mathbf{v}_{i-1} + \\eta \\nabla_w f(\\mathbf{w}_i) \\\\\n",
    "\\mathbf{w}_{i+1} &\\leftarrow \\mathbf{w}_i - \\mathbf{v}_{i} \n",
    "\\end{align}\n",
    "\n",
    "where we have a new momentum **hyperparamter** $0 \\le \\beta < 1$. If $\\beta = 0$, we have an ordinary gradient descent. By increasing $\\beta$, we increases the information saved from the previous steps.  Note that $\\beta \\approx 0.9$ gives us a memory of approximately 10 iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, ax3d = plot_function(np.linspace(-5, 5, 50), lambda x: f(x, A))\n",
    "\n",
    "# hyperparameters\n",
    "η = 0.5\n",
    "β  = 0.9\n",
    "num_iter = 10\n",
    "\n",
    "w = np.array([4.0,3.5])\n",
    "v = np.zeros(2)\n",
    "\n",
    "ax.plot(*w, marker='.', color='k', ms=15)  \n",
    "\n",
    "for i in range(num_iter):\n",
    "    \n",
    "    # keep a copy for plotting\n",
    "    w_old = np.copy(w)\n",
    "    \n",
    "    # perform the CM update\n",
    "    v = β*v + η*df_dw(w, A)\n",
    "    w -= v\n",
    "    \n",
    "    # plot\n",
    "    ax.plot([w_old[0], w[0]], [w_old[1], w[1]], marker='.', linestyle='-', color='k',lw=1) \n",
    "    ax3d.plot([w_old[0], w[0]], [w_old[1], w[1]], [f(w_old,A),f(w,A)], marker='.', linestyle='-', color='k',lw=1, zorder=100)\n",
    "\n",
    "    ax.set_title(f'$i={i}, w=[{w[0]:.2f},{w[1]:.2f}]$' + '\\n' + f'$f(w) = {f(w,A):.6f}$', fontsize=14);\n",
    "    display.display(fig)\n",
    "    display.clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADAM\n",
    "We will use the first and second moment of the gradient to update the learning rate for different parameters. We have\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{g}_i &= \\nabla_w f(\\mathbf{w}_i) \\\\\n",
    "\\mathbf{m}_i &= \\beta_1 \\mathbf{m}_{i-1} + (1-\\beta_1) \\mathbf{g}_i \\\\\n",
    "\\mathbf{v}_i &= \\beta_2 \\mathbf{v}_{i-1} +(1-\\beta_2)\\mathbf{g}_i^2  \\\\\n",
    "\\hat{\\mathbf{m}}_i &= \\frac{\\mathbf{m}_i}{1-(\\beta_1)^i} \\\\\n",
    "\\hat{\\mathbf{v}}_i &= \\frac{\\mathbf{v}_i}{1-(\\beta_2)^i}  \\\\\n",
    "\\mathbf{w}_{i+1} &=\\mathbf{w}_i - \\eta \\frac{\\hat{\\mathbf{m}}_i}{\\sqrt{\\hat{\\mathbf{v}}_i} +\\epsilon}, \\nonumber \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "where $\\beta_1$ and $\\beta_2$ set the memory lifetime of the first and second moment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "β1 = 0.9\n",
    "β2 = 0.999\n",
    "ϵ = 1.0E-8\n",
    "η = 0.5\n",
    "\n",
    "num_iter = 10\n",
    "\n",
    "ax.plot(*w, marker='.', color='k', ms=15)\n",
    "\n",
    "w = np.array([4.0,3.5])\n",
    "\n",
    "w_traj = np.zeros([num_iter,2])\n",
    "w_traj[0,:] = w\n",
    "fig, ax, ax3d = plot_function(np.linspace(-5, 5, 50), lambda x: f(x, A))\n",
    "\n",
    "m = np.zeros(2)\n",
    "v = np.zeros(2)\n",
    "\n",
    "for i in range(1,num_iter):\n",
    "\n",
    "    g = np.array(df_dw(w,A))\n",
    "    m = β1*m + (1-β1)*g\n",
    "    v = β2*v + (1-β2)*g*g\n",
    "\n",
    "    m̂ = m/(1-β1**i)\n",
    "    v̂ = v/(1-β2**i)\n",
    "\n",
    "    w_old = np.copy(w)\n",
    "\n",
    "    w = w - η*np.divide(m̂,np.sqrt(v̂) + ϵ)\n",
    "\n",
    "    # plot\n",
    "    ax.plot([w_old[0], w[0]], [w_old[1], w[1]], marker='.', linestyle='-', color='k',lw=1)\n",
    "    ax3d.plot([w_old[0], w[0]], [w_old[1], w[1]], [f(w_old,A),f(w,A)], marker='.', linestyle='-', color='k',lw=1, zorder=100)\n",
    "\n",
    "    ax.set_title(f'$i={i}, w=[{w[0]:.2f},{w[1]:.2f}]$' + '\\n' + f'$f(w) = {f(w,A):.6f}$', fontsize=14);\n",
    "\n",
    "    display.display(fig)\n",
    "    display.clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Sample Data\n",
    "\n",
    "First, let's look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.loadtxt('sample.txt', unpack=True)\n",
    "\n",
    "plt.plot(x,y, 'o')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to minimize the cost function: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{C} = \\frac{1}{2N} \\sum_{n=1}^N  \\left( F^{(n)} - y^{(n)} \\right)^2 = \\frac{1}{2N} \\lvert \\lvert \\vec{F} - \\vec{y}\\rvert\\rvert^2\n",
    "\\end{equation}\n",
    "\n",
    "which measures the difference between some data set $\\{(x^{(n)},y^{(n)})\\}_{n=1}^N$ to some proposed model $F$. Here, we guess that the our predicted model is of the form $F(x)=\\cos(w_0x)\\sin(w_1x)$. We want to find $w_0$ and $w_1$ that will minimize our cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted model\n",
    "def F(x,w):\n",
    "    return jnp.cos(w[0]*x)*jnp.sin(w[1]*x)\n",
    "\n",
    "#cost function\n",
    "def C(w,x,y):\n",
    "    return 0.5*jnp.average((F(x,w)-y)**2)\n",
    "\n",
    "dC_dw = grad(C,argnums=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, ax3d = plot_function(np.linspace(-4, 4, 50), lambda w: C(w,x,y))\n",
    "\n",
    "# hyperparameters\n",
    "η = 0.1\n",
    "num_iter = 100\n",
    "\n",
    "# let's start near the minimum\n",
    "w = np.array([2.5,2.5])\n",
    "\n",
    "# store the cost function during the trajectory\n",
    "C_traj = {}\n",
    "C_traj['GD'] = np.zeros([num_iter])\n",
    "\n",
    "ax.plot(*w, marker='.', color='k', ms=15)  \n",
    "\n",
    "for i in range(num_iter):\n",
    "\n",
    "    # we keep a copy of the previous version for plotting\n",
    "    w_old = np.copy(w)\n",
    "    C_traj['GD'][i] = C(w,x,y)\n",
    "    \n",
    "    # perform the GD update\n",
    "    w += -η*dC_dw(w,x,y)\n",
    "    \n",
    "    # plot\n",
    "# plot\n",
    "    if i % 10 == 0:\n",
    "        ax.plot([w_old[0], w[0]], [w_old[1], w[1]], marker='.', linestyle='-', color='k',lw=1)\n",
    "        ax3d.plot([w_old[0], w[0]], [w_old[1], w[1]], [C(w_old,x,y),C(w,x,y)], marker='.', linestyle='-', color='k',lw=1, zorder=100)\n",
    "        ax.set_title(fr'$w=[{w[0]:.2f},{w[1]:.2f}]$' + '\\n' + f'$C(w) = {C(w,x,y):.6f}$', fontsize=14);\n",
    "        display.display(fig)\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(C_traj['GD'], label='Gradient Descent')\n",
    "plt.xlabel('Iteration Step')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y, 'o', label='data', alpha=0.5)\n",
    "plt.plot(x,F(x,w),'-', label='fit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 #samples\n",
    "NB = 5 #batch\n",
    "idx = np.random.choice(N, NB, replace=False)\n",
    "\n",
    "# hyperparameters\n",
    "η = 0.1\n",
    "w = np.array([2.5,2.5])\n",
    "\n",
    "num_epoch = 5\n",
    "num_batch = int(N/NB)\n",
    "\n",
    "w_traj = np.zeros([num_epoch*num_batch,2])\n",
    "w_traj[0,:] = w\n",
    "\n",
    "C_traj['SGD'] = np.zeros([num_epoch*num_batch])\n",
    "\n",
    "fig, ax, ax3d = plot_function(np.linspace(-4, 4, 50), lambda w: C(w,x,y))\n",
    "\n",
    "ax.plot(*w, marker='.', color='k', ms=15)\n",
    "\n",
    "# each epoch includes all minibatches\n",
    "i = 0\n",
    "for epoch in range(num_epoch):\n",
    "    for batch in range(num_batch):\n",
    "\n",
    "        # get the batch\n",
    "        idx = np.random.choice(N, NB, replace=False)\n",
    "\n",
    "        # we keep a copy of the previous version for plotting\n",
    "        w_old = np.copy(w)\n",
    "        C_traj['SGD'][i] = C(w,x,y)\n",
    "\n",
    "        # perform the stocahstic GD update\n",
    "        w += -η*dC_dw(w,x[idx],y[idx])\n",
    "    \n",
    "        # plot\n",
    "        if i % 10 == 0:\n",
    "\n",
    "            ax.plot([w_old[0], w[0]], [w_old[1], w[1]], marker='.', linestyle='-', color='k',lw=1)\n",
    "            ax3d.plot([w_old[0], w[0]], [w_old[1], w[1]], [C(w_old,x,y),C(w,x,y)], marker='.', linestyle='-', color='k',lw=1, zorder=100)\n",
    "\n",
    "            ax.set_title(fr'$w=[{w[0]:.2f},{w[1]:.2f}]$' + '\\n' + f'$C(w) = {C(w,x,y):.6f}$', fontsize=14);\n",
    "            display.display(fig)\n",
    "            display.clear_output(wait=True)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(C_traj['GD'], label='Gradient Descent')\n",
    "plt.plot(C_traj['SGD'], label='Stochastic Gradient Descent')\n",
    "\n",
    "plt.xlabel('Iteration Step')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y, 'o', label='data', alpha=0.5)\n",
    "plt.plot(x,F(x,w),'-', label='fit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
